{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874a4af5",
   "metadata": {},
   "source": [
    "# Adjust the architecutre of pre-train model to our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03a2f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 08:47:16,605 - INFO - 🚀 Starting ViT Model Adjustment for Breast Cancer Detection\n",
      "2025-07-01 08:47:16,606 - INFO - Configuration initialized for breast_cancer_detection\n",
      "2025-07-01 08:47:16,607 - INFO - Target classes: 2 (normal, cancer)\n",
      "2025-07-01 08:47:16,607 - INFO - Device: cpu\n",
      "2025-07-01 08:47:16,607 - INFO - Starting ViT model adjustment process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 ViT Model Adjustment for Breast Cancer Detection\n",
      "============================================================\n",
      "This script will adjust the ViT model architecture for binary classification.\n",
      "The model will be ready for fine-tuning on your breast cancer dataset.\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e68ddfb5524d2c80d9013d0773e564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Adjustment Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 08:47:16,628 - INFO - Loading original ViT model and processor...\n",
      "2025-07-01 08:47:16,637 - INFO - Loading from local path: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/raw_model\n",
      "2025-07-01 08:47:16,884 - INFO - Original model loaded successfully\n",
      "2025-07-01 08:47:16,884 - INFO - Original number of classes: 1000\n",
      "2025-07-01 08:47:16,885 - INFO - Model architecture: ['ViTForImageClassification']\n",
      "2025-07-01 08:47:16,886 - INFO - \n",
      "==================================================\n",
      "2025-07-01 08:47:16,886 - INFO - Original Model Information\n",
      "2025-07-01 08:47:16,887 - INFO - ==================================================\n",
      "2025-07-01 08:47:16,888 - INFO - Architecture: ['ViTForImageClassification']\n",
      "2025-07-01 08:47:16,888 - INFO - Number of labels: 1000\n",
      "2025-07-01 08:47:16,889 - INFO - Hidden size: 768\n",
      "2025-07-01 08:47:16,889 - INFO - Number of attention heads: 12\n",
      "2025-07-01 08:47:16,890 - INFO - Number of layers: 12\n",
      "2025-07-01 08:47:16,890 - INFO - Image size: 224\n",
      "2025-07-01 08:47:16,891 - INFO - Patch size: 16\n",
      "2025-07-01 08:47:16,891 - INFO - Classifier input features: 768\n",
      "2025-07-01 08:47:16,892 - INFO - Classifier output features: 1000\n",
      "2025-07-01 08:47:16,893 - INFO - Total parameters: 86,567,656\n",
      "2025-07-01 08:47:16,893 - INFO - Trainable parameters: 86,567,656\n",
      "2025-07-01 08:47:16,893 - INFO - ==================================================\n",
      "\n",
      "2025-07-01 08:47:16,894 - INFO - Adjusting model architecture for breast cancer classification...\n",
      "2025-07-01 08:47:16,899 - INFO - Creating adjusted model with new configuration...\n",
      "2025-07-01 08:47:18,424 - INFO - Copying pre-trained weights...\n",
      "2025-07-01 08:47:20,193 - INFO - Copied weights for 198 layers\n",
      "2025-07-01 08:47:20,194 - INFO - Skipped 2 layers:\n",
      "2025-07-01 08:47:20,194 - INFO -   - classifier.weight (classifier layer)\n",
      "2025-07-01 08:47:20,195 - INFO -   - classifier.bias (classifier layer)\n",
      "2025-07-01 08:47:20,196 - INFO - Initializing new classifier layer...\n",
      "2025-07-01 08:47:20,197 - INFO - Classifier initialized - Input: 768, Output: 2\n",
      "2025-07-01 08:47:20,197 - INFO - Model architecture adjusted successfully!\n",
      "2025-07-01 08:47:20,198 - INFO - \n",
      "==================================================\n",
      "2025-07-01 08:47:20,199 - INFO - Adjusted Model Information\n",
      "2025-07-01 08:47:20,199 - INFO - ==================================================\n",
      "2025-07-01 08:47:20,200 - INFO - Architecture: ['ViTForImageClassification']\n",
      "2025-07-01 08:47:20,200 - INFO - Number of labels: 2\n",
      "2025-07-01 08:47:20,201 - INFO - Hidden size: 768\n",
      "2025-07-01 08:47:20,201 - INFO - Number of attention heads: 12\n",
      "2025-07-01 08:47:20,201 - INFO - Number of layers: 12\n",
      "2025-07-01 08:47:20,201 - INFO - Image size: 224\n",
      "2025-07-01 08:47:20,202 - INFO - Patch size: 16\n",
      "2025-07-01 08:47:20,202 - INFO - Classifier input features: 768\n",
      "2025-07-01 08:47:20,202 - INFO - Classifier output features: 2\n",
      "2025-07-01 08:47:20,203 - INFO - Total parameters: 85,800,194\n",
      "2025-07-01 08:47:20,203 - INFO - Trainable parameters: 85,800,194\n",
      "2025-07-01 08:47:20,203 - INFO - ==================================================\n",
      "\n",
      "2025-07-01 08:47:20,204 - INFO - Validating adjusted model...\n",
      "2025-07-01 08:47:20,322 - INFO - ✓ Model validation successful! Output shape: torch.Size([1, 2])\n",
      "2025-07-01 08:47:20,326 - INFO - ✓ Sample output probabilities: [0.18248295783996582, 0.8175170421600342]\n",
      "2025-07-01 08:47:20,354 - INFO - Created output directory: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:20,354 - INFO - Saving adjusted model to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:21,955 - INFO - ✓ Model saved successfully\n",
      "2025-07-01 08:47:22,000 - INFO - ✓ Processor saved successfully\n",
      "2025-07-01 08:47:22,024 - INFO - ✓ Metadata saved to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted/adjustment_metadata.json\n",
      "2025-07-01 08:47:22,056 - INFO - ✓ README created: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted/README.md\n",
      "2025-07-01 08:47:22,057 - INFO - Verifying saved model...\n",
      "2025-07-01 08:47:23,654 - INFO - ✅ Saved model verification successful!\n",
      "2025-07-01 08:47:23,656 - INFO - ✅ Model adjustment completed successfully!\n",
      "2025-07-01 08:47:23,657 - INFO - Adjusted model saved to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:23,657 - INFO - 🎉 Process completed successfully!\n",
      "2025-07-01 08:47:23,657 - INFO - 📁 Adjusted model location: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:23,658 - INFO - 📋 Next steps:\n",
      "2025-07-01 08:47:23,658 - INFO -    1. Use this adjusted model for fine-tuning on your dataset\n",
      "2025-07-01 08:47:23,658 - INFO -    2. The model is ready for training with your breast cancer data\n",
      "2025-07-01 08:47:23,659 - INFO -    3. Check the README.md file for usage instructions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS: Model adjusted and saved to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    ViTImageProcessor, \n",
    "    ViTForImageClassification,\n",
    "    ViTConfig\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('model_adjustment.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelAdjustmentConfig:\n",
    "    \"\"\"Configuration class for model architecture adjustment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model paths\n",
    "        self.base_model_path = '/Volumes/KODAK/folder 02/Brest_cancer_prediction/model/raw_model'\n",
    "        self.output_model_path = '/Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model'\n",
    "        self.adjusted_model_name = 'breast_cancer_vit_adjusted'\n",
    "        \n",
    "        # Task-specific parameters\n",
    "        self.num_classes = 2  # Binary classification: cancer/no cancer\n",
    "        self.task_name = 'breast_cancer_detection'\n",
    "        self.class_names = ['normal', 'cancer']\n",
    "        \n",
    "        # Model configuration\n",
    "        self.dropout_rate = 0.1\n",
    "        self.label_smoothing = 0.1\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        logger.info(f\"Configuration initialized for {self.task_name}\")\n",
    "        logger.info(f\"Target classes: {self.num_classes} ({', '.join(self.class_names)})\")\n",
    "        logger.info(f\"Device: {self.device}\")\n",
    "\n",
    "class ViTModelAdjuster:\n",
    "    \"\"\"Class for adjusting ViT model architecture for specific tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelAdjustmentConfig):\n",
    "        self.config = config\n",
    "        self.processor = None\n",
    "        self.original_model = None\n",
    "        self.adjusted_model = None\n",
    "        self.model_config = None\n",
    "        \n",
    "    def create_output_directory(self):\n",
    "        \"\"\"Create output directory structure\"\"\"\n",
    "        output_path = Path(self.config.output_model_path)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        adjusted_model_path = output_path / self.config.adjusted_model_name\n",
    "        adjusted_model_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Created output directory: {adjusted_model_path}\")\n",
    "        return str(adjusted_model_path)\n",
    "    \n",
    "    def load_original_model(self):\n",
    "        \"\"\"Load the original pre-trained model\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Loading original ViT model and processor...\")\n",
    "            \n",
    "            # Check if local model exists\n",
    "            if os.path.exists(self.config.base_model_path):\n",
    "                logger.info(f\"Loading from local path: {self.config.base_model_path}\")\n",
    "                \n",
    "                # Load processor\n",
    "                self.processor = ViTImageProcessor.from_pretrained(self.config.base_model_path)\n",
    "                \n",
    "                # Load model configuration\n",
    "                self.model_config = ViTConfig.from_pretrained(self.config.base_model_path)\n",
    "                \n",
    "                # Load original model\n",
    "                self.original_model = ViTForImageClassification.from_pretrained(\n",
    "                    self.config.base_model_path\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                logger.info(\"Local model not found. Loading from Hugging Face...\")\n",
    "                \n",
    "                # Load processor\n",
    "                self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "                \n",
    "                # Load model configuration\n",
    "                self.model_config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "                \n",
    "                # Load original model\n",
    "                self.original_model = ViTForImageClassification.from_pretrained(\n",
    "                    'google/vit-base-patch16-224'\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Original model loaded successfully\")\n",
    "            logger.info(f\"Original number of classes: {self.model_config.num_labels}\")\n",
    "            logger.info(f\"Model architecture: {self.model_config.architectures}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading original model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def print_model_info(self, model, title=\"Model Information\"):\n",
    "        \"\"\"Print detailed model information\"\"\"\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"{title}\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "        \n",
    "        # Model configuration\n",
    "        config = model.config\n",
    "        logger.info(f\"Architecture: {config.architectures}\")\n",
    "        logger.info(f\"Number of labels: {config.num_labels}\")\n",
    "        logger.info(f\"Hidden size: {config.hidden_size}\")\n",
    "        logger.info(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "        logger.info(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "        logger.info(f\"Image size: {config.image_size}\")\n",
    "        logger.info(f\"Patch size: {config.patch_size}\")\n",
    "        \n",
    "        # Classifier layer info\n",
    "        if hasattr(model, 'classifier'):\n",
    "            classifier = model.classifier\n",
    "            logger.info(f\"Classifier input features: {classifier.in_features}\")\n",
    "            logger.info(f\"Classifier output features: {classifier.out_features}\")\n",
    "        \n",
    "        # Model parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        logger.info(f\"Total parameters: {total_params:,}\")\n",
    "        logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        logger.info(f\"{'='*50}\\n\")\n",
    "    \n",
    "    def adjust_model_architecture(self):\n",
    "        \"\"\"Adjust model architecture for the target task\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Adjusting model architecture for breast cancer classification...\")\n",
    "            \n",
    "            # Create new configuration with adjusted parameters\n",
    "            new_config = ViTConfig.from_pretrained(\n",
    "                self.config.base_model_path if os.path.exists(self.config.base_model_path) \n",
    "                else 'google/vit-base-patch16-224'\n",
    "            )\n",
    "            \n",
    "            # Update configuration for the new task\n",
    "            new_config.num_labels = self.config.num_classes\n",
    "            new_config.id2label = {i: label for i, label in enumerate(self.config.class_names)}\n",
    "            new_config.label2id = {label: i for i, label in enumerate(self.config.class_names)}\n",
    "            new_config.problem_type = \"single_label_classification\"\n",
    "            \n",
    "            # Update dropout rates for better regularization\n",
    "            new_config.hidden_dropout_prob = self.config.hidden_dropout_prob\n",
    "            new_config.attention_probs_dropout_prob = self.config.attention_probs_dropout_prob\n",
    "            \n",
    "            # Add task-specific metadata\n",
    "            new_config.task_specific_params = {\n",
    "                \"task_name\": self.config.task_name,\n",
    "                \"num_classes\": self.config.num_classes,\n",
    "                \"class_names\": self.config.class_names,\n",
    "                \"adjustment_date\": datetime.now().isoformat(),\n",
    "                \"base_model\": \"google/vit-base-patch16-224\"\n",
    "            }\n",
    "            \n",
    "            logger.info(\"Creating adjusted model with new configuration...\")\n",
    "            \n",
    "            # Create new model with adjusted configuration\n",
    "            self.adjusted_model = ViTForImageClassification(new_config)\n",
    "            \n",
    "            # Copy weights from original model (except classifier layer)\n",
    "            self.copy_pretrained_weights()\n",
    "            \n",
    "            # Initialize new classifier layer\n",
    "            self.initialize_classifier_layer()\n",
    "            \n",
    "            logger.info(\"Model architecture adjusted successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adjusting model architecture: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def copy_pretrained_weights(self):\n",
    "        \"\"\"Copy weights from original model to adjusted model (except classifier)\"\"\"\n",
    "        logger.info(\"Copying pre-trained weights...\")\n",
    "        \n",
    "        # Get state dictionaries\n",
    "        original_state_dict = self.original_model.state_dict()\n",
    "        adjusted_state_dict = self.adjusted_model.state_dict()\n",
    "        \n",
    "        # Copy all weights except classifier\n",
    "        copied_layers = []\n",
    "        skipped_layers = []\n",
    "        \n",
    "        for name, param in original_state_dict.items():\n",
    "            if name in adjusted_state_dict and not name.startswith('classifier'):\n",
    "                if param.shape == adjusted_state_dict[name].shape:\n",
    "                    adjusted_state_dict[name].copy_(param)\n",
    "                    copied_layers.append(name)\n",
    "                else:\n",
    "                    skipped_layers.append(f\"{name} (shape mismatch)\")\n",
    "            elif name.startswith('classifier'):\n",
    "                skipped_layers.append(f\"{name} (classifier layer)\")\n",
    "            else:\n",
    "                skipped_layers.append(f\"{name} (not found in target)\")\n",
    "        \n",
    "        # Load the updated state dict\n",
    "        self.adjusted_model.load_state_dict(adjusted_state_dict)\n",
    "        \n",
    "        logger.info(f\"Copied weights for {len(copied_layers)} layers\")\n",
    "        logger.info(f\"Skipped {len(skipped_layers)} layers:\")\n",
    "        for layer in skipped_layers[:5]:  # Show first 5 skipped layers\n",
    "            logger.info(f\"  - {layer}\")\n",
    "        if len(skipped_layers) > 5:\n",
    "            logger.info(f\"  ... and {len(skipped_layers) - 5} more\")\n",
    "    \n",
    "    def initialize_classifier_layer(self):\n",
    "        \"\"\"Initialize the new classifier layer with proper weights\"\"\"\n",
    "        logger.info(\"Initializing new classifier layer...\")\n",
    "        \n",
    "        # Get the classifier layer\n",
    "        classifier = self.adjusted_model.classifier\n",
    "        \n",
    "        # Initialize with Xavier/Glorot initialization\n",
    "        nn.init.xavier_uniform_(classifier.weight)\n",
    "        nn.init.constant_(classifier.bias, 0)\n",
    "        \n",
    "        logger.info(f\"Classifier initialized - Input: {classifier.in_features}, Output: {classifier.out_features}\")\n",
    "    \n",
    "    def validate_adjusted_model(self):\n",
    "        \"\"\"Validate the adjusted model\"\"\"\n",
    "        logger.info(\"Validating adjusted model...\")\n",
    "        \n",
    "        try:\n",
    "            # Create dummy input\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            \n",
    "            # Test forward pass\n",
    "            self.adjusted_model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.adjusted_model(pixel_values=dummy_input)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # Validate output shape\n",
    "            expected_shape = (1, self.config.num_classes)\n",
    "            actual_shape = logits.shape\n",
    "            \n",
    "            if actual_shape == expected_shape:\n",
    "                logger.info(f\"✓ Model validation successful! Output shape: {actual_shape}\")\n",
    "                \n",
    "                # Test probability distribution\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                logger.info(f\"✓ Sample output probabilities: {probabilities.squeeze().tolist()}\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"✗ Model validation failed! Expected shape: {expected_shape}, Got: {actual_shape}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Model validation failed with error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def save_adjusted_model(self):\n",
    "        \"\"\"Save the adjusted model and processor\"\"\"\n",
    "        try:\n",
    "            # Create output directory\n",
    "            output_dir = self.create_output_directory()\n",
    "            \n",
    "            logger.info(f\"Saving adjusted model to: {output_dir}\")\n",
    "            \n",
    "            # Save model\n",
    "            self.adjusted_model.save_pretrained(output_dir)\n",
    "            logger.info(\"✓ Model saved successfully\")\n",
    "            \n",
    "            # Save processor\n",
    "            self.processor.save_pretrained(output_dir)\n",
    "            logger.info(\"✓ Processor saved successfully\")\n",
    "            \n",
    "            # Save adjustment metadata\n",
    "            metadata = {\n",
    "                \"adjustment_info\": {\n",
    "                    \"original_model\": \"google/vit-base-patch16-224\",\n",
    "                    \"task\": self.config.task_name,\n",
    "                    \"num_classes\": self.config.num_classes,\n",
    "                    \"class_names\": self.config.class_names,\n",
    "                    \"adjustment_date\": datetime.now().isoformat()\n",
    "                },\n",
    "                \"model_config\": {\n",
    "                    \"hidden_size\": self.adjusted_model.config.hidden_size,\n",
    "                    \"num_attention_heads\": self.adjusted_model.config.num_attention_heads,\n",
    "                    \"num_hidden_layers\": self.adjusted_model.config.num_hidden_layers,\n",
    "                    \"image_size\": self.adjusted_model.config.image_size,\n",
    "                    \"patch_size\": self.adjusted_model.config.patch_size,\n",
    "                    \"num_labels\": self.adjusted_model.config.num_labels\n",
    "                },\n",
    "                \"usage_instructions\": {\n",
    "                    \"loading\": \"Use ViTForImageClassification.from_pretrained() to load this model\",\n",
    "                    \"processor\": \"Use ViTImageProcessor.from_pretrained() to load the processor\",\n",
    "                    \"input_size\": \"224x224 RGB images\",\n",
    "                    \"output\": f\"{self.config.num_classes} class probabilities\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            metadata_path = os.path.join(output_dir, 'adjustment_metadata.json')\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"✓ Metadata saved to: {metadata_path}\")\n",
    "            \n",
    "            # Create README file\n",
    "            readme_content = self.create_readme_content(output_dir)\n",
    "            readme_path = os.path.join(output_dir, 'README.md')\n",
    "            with open(readme_path, 'w') as f:\n",
    "                f.write(readme_content)\n",
    "            \n",
    "            logger.info(f\"✓ README created: {readme_path}\")\n",
    "            \n",
    "            return output_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving adjusted model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_readme_content(self, model_path):\n",
    "        \"\"\"Create README content for the adjusted model\"\"\"\n",
    "        readme = f\"\"\"# Breast Cancer Detection ViT Model (Adjusted)\n",
    "\n",
    "## Model Information\n",
    "- **Base Model**: google/vit-base-patch16-224\n",
    "- **Task**: Binary classification for breast cancer detection\n",
    "- **Classes**: {self.config.num_classes} ({', '.join(self.config.class_names)})\n",
    "- **Adjustment Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Model Architecture\n",
    "- **Input Size**: 224x224 RGB images\n",
    "- **Hidden Size**: {self.adjusted_model.config.hidden_size}\n",
    "- **Attention Heads**: {self.adjusted_model.config.num_attention_heads}\n",
    "- **Layers**: {self.adjusted_model.config.num_hidden_layers}\n",
    "- **Parameters**: {sum(p.numel() for p in self.adjusted_model.parameters()):,}\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Loading the Model\n",
    "```python\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Load model and processor\n",
    "model = ViTForImageClassification.from_pretrained('{model_path}')\n",
    "processor = ViTImageProcessor.from_pretrained('{model_path}')\n",
    "```\n",
    "\n",
    "### Inference Example\n",
    "```python\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess image\n",
    "image = Image.open('path/to/mri_image.jpg')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class_id = predictions.argmax().item()\n",
    "predicted_class = {self.config.class_names}[predicted_class_id]\n",
    "confidence = predictions[0][predicted_class_id].item()\n",
    "\n",
    "print(f\"Prediction: {{predicted_class}} (Confidence: {{confidence:.4f}})\")\n",
    "```\n",
    "\n",
    "## Fine-tuning Ready\n",
    "This model is ready for fine-tuning on your breast cancer dataset. The classifier layer has been properly initialized for binary classification.\n",
    "\n",
    "## Notes\n",
    "- The model uses the original ViT-Base architecture with an adjusted classifier layer\n",
    "- All pre-trained weights from ImageNet are preserved except for the final classification layer\n",
    "- The classifier layer has been randomly initialized and needs fine-tuning on your dataset\n",
    "\"\"\"\n",
    "        return readme\n",
    "    \n",
    "    def run_adjustment_process(self):\n",
    "        \"\"\"Run the complete model adjustment process\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting ViT model adjustment process...\")\n",
    "            \n",
    "            # Step 1: Load original model\n",
    "            with tqdm(total=5, desc=\"Model Adjustment Progress\") as pbar:\n",
    "                self.load_original_model()\n",
    "                pbar.set_description(\"Original model loaded\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Print original model info\n",
    "                self.print_model_info(self.original_model, \"Original Model Information\")\n",
    "                \n",
    "                # Step 2: Adjust architecture\n",
    "                self.adjust_model_architecture()\n",
    "                pbar.set_description(\"Architecture adjusted\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Print adjusted model info\n",
    "                self.print_model_info(self.adjusted_model, \"Adjusted Model Information\")\n",
    "                \n",
    "                # Step 3: Validate adjusted model\n",
    "                validation_success = self.validate_adjusted_model()\n",
    "                if not validation_success:\n",
    "                    raise Exception(\"Model validation failed!\")\n",
    "                pbar.set_description(\"Model validated\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Step 4: Save adjusted model\n",
    "                output_path = self.save_adjusted_model()\n",
    "                pbar.set_description(\"Model saved\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Step 5: Final verification\n",
    "                self.verify_saved_model(output_path)\n",
    "                pbar.set_description(\"Verification complete\")\n",
    "                pbar.update(1)\n",
    "            \n",
    "            logger.info(\"✅ Model adjustment completed successfully!\")\n",
    "            logger.info(f\"Adjusted model saved to: {output_path}\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Model adjustment failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def verify_saved_model(self, model_path):\n",
    "        \"\"\"Verify that the saved model can be loaded correctly\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Verifying saved model...\")\n",
    "            \n",
    "            # Load the saved model\n",
    "            loaded_model = ViTForImageClassification.from_pretrained(model_path)\n",
    "            loaded_processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "            \n",
    "            # Test with dummy input\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            \n",
    "            loaded_model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = loaded_model(pixel_values=dummy_input)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            if logits.shape == (1, self.config.num_classes):\n",
    "                logger.info(\"✅ Saved model verification successful!\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"❌ Saved model verification failed!\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error verifying saved model: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    logger.info(\"🚀 Starting ViT Model Adjustment for Breast Cancer Detection\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        config = ModelAdjustmentConfig()\n",
    "        \n",
    "        # Initialize model adjuster\n",
    "        adjuster = ViTModelAdjuster(config)\n",
    "        \n",
    "        # Run adjustment process\n",
    "        output_path = adjuster.run_adjustment_process()\n",
    "        \n",
    "        logger.info(\"🎉 Process completed successfully!\")\n",
    "        logger.info(f\"📁 Adjusted model location: {output_path}\")\n",
    "        logger.info(\"📋 Next steps:\")\n",
    "        logger.info(\"   1. Use this adjusted model for fine-tuning on your dataset\")\n",
    "        logger.info(\"   2. The model is ready for training with your breast cancer data\")\n",
    "        logger.info(\"   3. Check the README.md file for usage instructions\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"💥 Process failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# For Jupyter Notebook usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔬 ViT Model Adjustment for Breast Cancer Detection\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"This script will adjust the ViT model architecture for binary classification.\")\n",
    "    print(\"The model will be ready for fine-tuning on your breast cancer dataset.\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run the adjustment process\n",
    "    try:\n",
    "        output_path = main()\n",
    "        print(f\"\\n✅ SUCCESS: Model adjusted and saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {str(e)}\")\n",
    "        print(\"Please check the logs for more details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0fbd7",
   "metadata": {},
   "source": [
    "## Try to unfreeze the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3c3622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏥 ViT Breast Cancer Detection - Freeze/Unfreeze Manager\n",
      "============================================================\n",
      "Loading adjusted ViT model...\n",
      "✓ Model loaded successfully from /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "Model: ViTForImageClassification\n",
      "Total Parameters: 85,800,194\n",
      "Trainable Parameters: 85,800,194\n",
      "Frozen Parameters: 0\n",
      "Trainable Percentage: 100.00%\n",
      "\n",
      "Layer-wise Parameter Status:\n",
      "--------------------------------------------------\n",
      "vit.embeddings.cls_token                 | Trainable  |      768 params\n",
      "vit.embeddings.position_embeddings       | Trainable  |  151,296 params\n",
      "vit.embeddings.patch_embeddings.projection.weight | Trainable  |  589,824 params\n",
      "vit.embeddings.patch_embeddings.projection.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.0.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.0.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.0.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.0.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.0.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.0.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.0.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.0.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.0.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.0.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.0.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.0.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.0.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.0.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.0.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.0.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.1.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.1.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.1.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.1.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.1.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.1.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.1.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.1.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.1.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.1.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.1.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.1.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.1.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.1.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.1.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.1.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.2.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.2.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.2.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.2.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.2.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.2.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.2.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.2.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.2.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.2.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.2.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.2.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.2.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.2.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.2.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.2.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.3.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.3.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.3.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.3.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.3.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.3.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.3.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.3.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.3.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.3.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.3.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.3.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.3.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.3.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.3.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.3.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.4.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.4.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.4.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.4.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.4.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.4.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.4.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.4.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.4.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.4.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.4.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.4.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.4.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.4.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.4.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.4.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.5.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.5.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.5.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.5.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.5.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.5.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.5.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.5.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.5.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.5.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.5.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.5.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.5.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.5.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.5.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.5.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.6.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.6.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.6.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.6.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.6.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.6.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.6.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.6.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.6.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.6.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.6.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.6.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.6.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.6.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.6.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.6.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.7.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.7.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.7.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.7.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.7.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.7.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.7.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.7.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.7.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.7.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.7.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.7.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.7.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.7.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.7.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.7.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.8.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.8.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.8.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.8.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.8.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.8.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.8.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.8.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.8.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.8.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.8.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.8.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.8.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.8.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.8.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.8.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.9.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.9.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.9.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.10.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.10.output.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.10.output.dense.bias   | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.11.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.11.output.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.11.output.dense.bias   | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.layernorm.weight                     | Trainable  |      768 params\n",
      "vit.layernorm.bias                       | Trainable  |      768 params\n",
      "classifier.weight                        | Trainable  |    1,536 params\n",
      "classifier.bias                          | Trainable  |        2 params\n",
      "\n",
      "============================================================\n",
      "STRATEGY RECOMMENDATION\n",
      "============================================================\n",
      "Dataset Size: 64,500 images\n",
      "Task: Breast Cancer Detection (Medical)\n",
      "Recommended Strategy: Unfreeze Last 3\n",
      "Reason: Large dataset - unfreeze more layers for better performance\n",
      "\n",
      "Note: For medical imaging, conservative approaches often work better!\n",
      "\n",
      "⚡ Applying recommended strategy: unfreeze_last_3\n",
      "\n",
      "Applying UNFREEZE LAST 3 LAYERS strategy...\n",
      "- Freezing: Early ViT layers\n",
      "- Unfreezing: Last 3 encoder layers + classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layers: 100%|██████████████████████████████| 200/200 [00:02<00:00, 77.65it/s, Layer=classifier, Status=TRAINABLE, Frozen=150, Trainable=50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Unfreeze Last Layers completed!\n",
      "  - Frozen layers: 150\n",
      "  - Trainable layers: 50\n",
      "\n",
      "============================================================\n",
      "UPDATED MODEL SUMMARY\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "Model: ViTForImageClassification\n",
      "Total Parameters: 85,800,194\n",
      "Trainable Parameters: 21,265,154\n",
      "Frozen Parameters: 64,535,040\n",
      "Trainable Percentage: 24.78%\n",
      "\n",
      "Layer-wise Parameter Status:\n",
      "--------------------------------------------------\n",
      "vit.embeddings.cls_token                 | Frozen     |      768 params\n",
      "vit.embeddings.position_embeddings       | Frozen     |  151,296 params\n",
      "vit.embeddings.patch_embeddings.projection.weight | Frozen     |  589,824 params\n",
      "vit.embeddings.patch_embeddings.projection.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.0.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.0.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.0.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.0.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.0.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.0.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.0.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.0.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.0.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.0.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.0.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.0.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.0.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.0.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.0.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.0.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.1.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.1.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.1.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.1.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.1.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.1.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.1.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.1.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.1.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.1.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.1.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.1.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.1.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.1.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.1.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.1.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.2.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.2.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.2.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.2.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.2.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.2.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.2.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.2.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.2.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.2.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.2.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.2.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.2.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.2.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.2.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.2.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.3.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.3.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.3.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.3.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.3.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.3.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.3.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.3.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.3.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.3.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.3.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.3.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.3.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.3.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.3.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.3.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.4.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.4.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.4.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.4.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.4.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.4.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.4.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.4.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.4.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.4.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.4.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.4.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.4.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.4.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.4.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.4.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.5.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.5.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.5.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.5.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.5.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.5.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.5.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.5.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.5.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.5.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.5.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.5.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.5.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.5.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.5.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.5.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.6.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.6.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.6.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.6.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.6.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.6.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.6.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.6.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.6.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.6.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.6.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.6.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.6.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.6.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.6.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.6.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.7.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.7.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.7.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.7.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.7.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.7.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.7.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.7.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.7.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.7.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.7.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.7.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.7.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.7.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.7.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.7.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.8.attention.attention.query.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.8.attention.attention.query.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.8.attention.attention.key.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.8.attention.attention.key.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.8.attention.attention.value.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.8.attention.attention.value.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.8.attention.output.dense.weight | Frozen     |  589,824 params\n",
      "vit.encoder.layer.8.attention.output.dense.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.8.intermediate.dense.weight | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.8.intermediate.dense.bias | Frozen     |    3,072 params\n",
      "vit.encoder.layer.8.output.dense.weight  | Frozen     | 2,359,296 params\n",
      "vit.encoder.layer.8.output.dense.bias    | Frozen     |      768 params\n",
      "vit.encoder.layer.8.layernorm_before.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.8.layernorm_before.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.8.layernorm_after.weight | Frozen     |      768 params\n",
      "vit.encoder.layer.8.layernorm_after.bias | Frozen     |      768 params\n",
      "vit.encoder.layer.9.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.9.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.9.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.9.output.dense.weight  | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.9.output.dense.bias    | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.9.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.10.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.10.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.10.output.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.10.output.dense.bias   | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.10.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.attention.query.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.attention.query.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.attention.key.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.attention.key.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.attention.value.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.attention.value.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.attention.output.dense.weight | Trainable  |  589,824 params\n",
      "vit.encoder.layer.11.attention.output.dense.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.intermediate.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.11.intermediate.dense.bias | Trainable  |    3,072 params\n",
      "vit.encoder.layer.11.output.dense.weight | Trainable  | 2,359,296 params\n",
      "vit.encoder.layer.11.output.dense.bias   | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_before.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_before.bias | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_after.weight | Trainable  |      768 params\n",
      "vit.encoder.layer.11.layernorm_after.bias | Trainable  |      768 params\n",
      "vit.layernorm.weight                     | Frozen     |      768 params\n",
      "vit.layernorm.bias                       | Frozen     |      768 params\n",
      "classifier.weight                        | Trainable  |    1,536 params\n",
      "classifier.bias                          | Trainable  |        2 params\n",
      "\n",
      "Saving model to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_unfreeze_last_3_20250701_091544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model: 100%|██████████████████████████████| 3/3 [00:02<00:00,  1.00it/s, Creating documentation...]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved successfully!\n",
      "  - Model directory: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_unfreeze_last_3_20250701_091544\n",
      "  - Strategy: unfreeze_last_3\n",
      "\n",
      "🎉 Process completed successfully!\n",
      "📁 Model saved at: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_unfreeze_last_3_20250701_091544\n",
      "🚀 Ready for fine-tuning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ViTFreezeManager:\n",
    "    def __init__(self, model_path, save_path):\n",
    "        \"\"\"\n",
    "        Initialize the ViT Freeze Manager\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the adjusted model\n",
    "            save_path: Path to save the processed model\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.save_path = save_path\n",
    "        self.model = None\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the adjusted ViT model\"\"\"\n",
    "        print(\"Loading adjusted ViT model...\")\n",
    "        try:\n",
    "            self.model = ViTForImageClassification.from_pretrained(self.model_path)\n",
    "            print(f\"✓ Model loaded successfully from {self.model_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"Get model summary and layer information\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model not loaded!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Basic model info\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        frozen_params = total_params - trainable_params\n",
    "        \n",
    "        print(f\"Model: {self.model.__class__.__name__}\")\n",
    "        print(f\"Total Parameters: {total_params:,}\")\n",
    "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "        print(f\"Frozen Parameters: {frozen_params:,}\")\n",
    "        print(f\"Trainable Percentage: {(trainable_params/total_params)*100:.2f}%\")\n",
    "        \n",
    "        # Layer-wise breakdown\n",
    "        print(f\"\\nLayer-wise Parameter Status:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            status = \"Trainable\" if param.requires_grad else \"Frozen\"\n",
    "            print(f\"{name:<40} | {status:<10} | {param.numel():>8,} params\")\n",
    "    \n",
    "    def freeze_backbone_only(self):\n",
    "        \"\"\"\n",
    "        Freeze only the ViT backbone (feature extractor), keep classifier trainable\n",
    "        Recommended for transfer learning with limited data\n",
    "        \"\"\"\n",
    "        print(\"\\nApplying FREEZE BACKBONE strategy...\")\n",
    "        print(\"- Freezing: ViT backbone (embeddings + encoder)\")\n",
    "        print(\"- Keeping trainable: Classifier head\")\n",
    "        \n",
    "        frozen_count = 0\n",
    "        trainable_count = 0\n",
    "        \n",
    "        with tqdm(total=len(list(self.model.named_parameters())), \n",
    "                  desc=\"Processing layers\", \n",
    "                  bar_format='{l_bar}{bar:30}{r_bar}') as pbar:\n",
    "            \n",
    "            for name, param in self.model.named_parameters():\n",
    "                # Freeze backbone components (embeddings and encoder)\n",
    "                if any(component in name for component in ['embeddings', 'encoder']):\n",
    "                    param.requires_grad = False\n",
    "                    frozen_count += 1\n",
    "                    status = \"FROZEN\"\n",
    "                else:\n",
    "                    # Keep classifier and layer norm trainable\n",
    "                    param.requires_grad = True\n",
    "                    trainable_count += 1\n",
    "                    status = \"TRAINABLE\"\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Layer': name.split('.')[0], \n",
    "                    'Status': status,\n",
    "                    'Frozen': frozen_count,\n",
    "                    'Trainable': trainable_count\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                time.sleep(0.01)  # Small delay for visual effect\n",
    "        \n",
    "        print(f\"\\n✓ Freeze Backbone completed!\")\n",
    "        print(f\"  - Frozen layers: {frozen_count}\")\n",
    "        print(f\"  - Trainable layers: {trainable_count}\")\n",
    "        \n",
    "        return \"freeze_backbone\"\n",
    "    \n",
    "    def unfreeze_last_layers(self, num_layers=2):\n",
    "        \"\"\"\n",
    "        Unfreeze the last N encoder layers + classifier\n",
    "        Recommended for fine-tuning with sufficient data\n",
    "        \"\"\"\n",
    "        print(f\"\\nApplying UNFREEZE LAST {num_layers} LAYERS strategy...\")\n",
    "        print(f\"- Freezing: Early ViT layers\")\n",
    "        print(f\"- Unfreezing: Last {num_layers} encoder layers + classifier\")\n",
    "        \n",
    "        # First freeze everything\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        frozen_count = 0\n",
    "        trainable_count = 0\n",
    "        \n",
    "        with tqdm(total=len(list(self.model.named_parameters())), \n",
    "                  desc=\"Processing layers\", \n",
    "                  bar_format='{l_bar}{bar:30}{r_bar}') as pbar:\n",
    "            \n",
    "            for name, param in self.model.named_parameters():\n",
    "                should_unfreeze = False\n",
    "                \n",
    "                # Always unfreeze classifier\n",
    "                if 'classifier' in name:\n",
    "                    should_unfreeze = True\n",
    "                \n",
    "                # Unfreeze last N encoder layers\n",
    "                if 'encoder.layer' in name:\n",
    "                    layer_num = int(name.split('encoder.layer.')[1].split('.')[0])\n",
    "                    total_layers = self.model.config.num_hidden_layers\n",
    "                    if layer_num >= (total_layers - num_layers):\n",
    "                        should_unfreeze = True\n",
    "                \n",
    "                if should_unfreeze:\n",
    "                    param.requires_grad = True\n",
    "                    trainable_count += 1\n",
    "                    status = \"TRAINABLE\"\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "                    frozen_count += 1\n",
    "                    status = \"FROZEN\"\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Layer': name.split('.')[0], \n",
    "                    'Status': status,\n",
    "                    'Frozen': frozen_count,\n",
    "                    'Trainable': trainable_count\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                time.sleep(0.01)\n",
    "        \n",
    "        print(f\"\\n✓ Unfreeze Last Layers completed!\")\n",
    "        print(f\"  - Frozen layers: {frozen_count}\")\n",
    "        print(f\"  - Trainable layers: {trainable_count}\")\n",
    "        \n",
    "        return f\"unfreeze_last_{num_layers}\"\n",
    "    \n",
    "    def full_unfreeze(self):\n",
    "        \"\"\"\n",
    "        Unfreeze all layers for complete fine-tuning\n",
    "        Use with caution - requires large dataset\n",
    "        \"\"\"\n",
    "        print(\"\\nApplying FULL UNFREEZE strategy...\")\n",
    "        print(\"- Unfreezing: All model parameters\")\n",
    "        \n",
    "        trainable_count = 0\n",
    "        \n",
    "        with tqdm(total=len(list(self.model.named_parameters())), \n",
    "                  desc=\"Unfreezing all layers\", \n",
    "                  bar_format='{l_bar}{bar:30}{r_bar}') as pbar:\n",
    "            \n",
    "            for name, param in self.model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "                trainable_count += 1\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Layer': name.split('.')[0], \n",
    "                    'Status': 'TRAINABLE',\n",
    "                    'Count': trainable_count\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                time.sleep(0.01)\n",
    "        \n",
    "        print(f\"\\n✓ Full Unfreeze completed!\")\n",
    "        print(f\"  - All {trainable_count} layers are trainable\")\n",
    "        \n",
    "        return \"full_unfreeze\"\n",
    "    \n",
    "    def save_model(self, strategy_name):\n",
    "        \"\"\"Save the processed model with strategy information\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_dir = os.path.join(self.save_path, f\"breast_cancer_vit_{strategy_name}_{timestamp}\")\n",
    "        \n",
    "        print(f\"\\nSaving model to: {model_dir}\")\n",
    "        \n",
    "        # Create directory\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model with progress bar\n",
    "        with tqdm(total=3, desc=\"Saving model\", bar_format='{l_bar}{bar:30}{r_bar}') as pbar:\n",
    "            \n",
    "            # Save model\n",
    "            pbar.set_postfix_str(\"Saving model weights...\")\n",
    "            self.model.save_pretrained(model_dir)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Save strategy info\n",
    "            pbar.set_postfix_str(\"Saving strategy info...\")\n",
    "            strategy_info = {\n",
    "                \"strategy\": strategy_name,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"total_parameters\": sum(p.numel() for p in self.model.parameters()),\n",
    "                \"trainable_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "                \"model_type\": \"ViT-Base-Patch16-224\",\n",
    "                \"task\": \"breast_cancer_detection\",\n",
    "                \"dataset_size\": \"64500_images\"\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(model_dir, \"strategy_info.json\"), 'w') as f:\n",
    "                json.dump(strategy_info, f, indent=2)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Create README\n",
    "            pbar.set_postfix_str(\"Creating documentation...\")\n",
    "            readme_content = f\"\"\"# Breast Cancer Detection Model - {strategy_name.title()}\n",
    "\n",
    "## Model Information\n",
    "- **Base Model**: google/vit-base-patch16-224\n",
    "- **Task**: Breast Cancer Detection\n",
    "- **Strategy**: {strategy_name.replace('_', ' ').title()}\n",
    "- **Created**: {timestamp}\n",
    "- **Dataset Size**: 64,500 images\n",
    "- **Image Size**: 224x224\n",
    "\n",
    "## Strategy Details\n",
    "\"\"\"\n",
    "            if \"freeze_backbone\" in strategy_name:\n",
    "                readme_content += \"\"\"\n",
    "- **Frozen**: ViT backbone (embeddings + encoder layers)\n",
    "- **Trainable**: Classification head only\n",
    "- **Recommended for**: Limited data, quick training\n",
    "\"\"\"\n",
    "            elif \"unfreeze_last\" in strategy_name:\n",
    "                readme_content += f\"\"\"\n",
    "- **Frozen**: Early ViT layers\n",
    "- **Trainable**: Last {strategy_name.split('_')[-1]} encoder layers + classifier\n",
    "- **Recommended for**: Moderate data, balanced approach\n",
    "\"\"\"\n",
    "            elif \"full_unfreeze\" in strategy_name:\n",
    "                readme_content += \"\"\"\n",
    "- **Frozen**: None\n",
    "- **Trainable**: All layers\n",
    "- **Recommended for**: Large dataset, full fine-tuning\n",
    "\"\"\"\n",
    "            \n",
    "            readme_content += f\"\"\"\n",
    "## Parameters\n",
    "- **Total Parameters**: {strategy_info['total_parameters']:,}\n",
    "- **Trainable Parameters**: {strategy_info['trainable_parameters']:,}\n",
    "- **Frozen Parameters**: {strategy_info['total_parameters'] - strategy_info['trainable_parameters']:,}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import ViTForImageClassification\n",
    "model = ViTForImageClassification.from_pretrained('{model_dir}')\n",
    "```\n",
    "\"\"\"\n",
    "            \n",
    "            with open(os.path.join(model_dir, \"README.md\"), 'w') as f:\n",
    "                f.write(readme_content)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        print(f\"✓ Model saved successfully!\")\n",
    "        print(f\"  - Model directory: {model_dir}\")\n",
    "        print(f\"  - Strategy: {strategy_name}\")\n",
    "        \n",
    "        return model_dir\n",
    "\n",
    "def recommend_strategy(dataset_size=64500):\n",
    "    \"\"\"Recommend the best strategy based on dataset size and task\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STRATEGY RECOMMENDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if dataset_size < 10000:\n",
    "        recommendation = \"freeze_backbone\"\n",
    "        reason = \"Small dataset - freeze backbone to prevent overfitting\"\n",
    "    elif dataset_size < 50000:\n",
    "        recommendation = \"unfreeze_last_2\"\n",
    "        reason = \"Medium dataset - unfreeze last layers for better adaptation\"\n",
    "    else:\n",
    "        recommendation = \"unfreeze_last_3\"\n",
    "        reason = \"Large dataset - unfreeze more layers for better performance\"\n",
    "    \n",
    "    print(f\"Dataset Size: {dataset_size:,} images\")\n",
    "    print(f\"Task: Breast Cancer Detection (Medical)\")\n",
    "    print(f\"Recommended Strategy: {recommendation.replace('_', ' ').title()}\")\n",
    "    print(f\"Reason: {reason}\")\n",
    "    print(\"\\nNote: For medical imaging, conservative approaches often work better!\")\n",
    "    \n",
    "    return recommendation\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🏥 ViT Breast Cancer Detection - Freeze/Unfreeze Manager\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Paths\n",
    "    model_path = \"/Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\"\n",
    "    save_path = \"/Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model\"\n",
    "    \n",
    "    # Initialize manager\n",
    "    manager = ViTFreezeManager(model_path, save_path)\n",
    "    \n",
    "    # Load model\n",
    "    if not manager.load_model():\n",
    "        return\n",
    "    \n",
    "    # Show initial model summary\n",
    "    manager.get_model_summary()\n",
    "    \n",
    "    # Get recommendation\n",
    "    recommended_strategy = recommend_strategy(64500)\n",
    "    \n",
    "    # Apply strategy based on recommendation\n",
    "    print(f\"\\n⚡ Applying recommended strategy: {recommended_strategy}\")\n",
    "    \n",
    "    if recommended_strategy == \"freeze_backbone\":\n",
    "        strategy_applied = manager.freeze_backbone_only()\n",
    "    elif \"unfreeze_last\" in recommended_strategy:\n",
    "        num_layers = int(recommended_strategy.split('_')[-1])\n",
    "        strategy_applied = manager.unfreeze_last_layers(num_layers)\n",
    "    else:\n",
    "        strategy_applied = manager.full_unfreeze()\n",
    "    \n",
    "    # Show updated model summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UPDATED MODEL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    manager.get_model_summary()\n",
    "    \n",
    "    # Save model\n",
    "    model_dir = manager.save_model(strategy_applied)\n",
    "    \n",
    "    print(f\"\\n🎉 Process completed successfully!\")\n",
    "    print(f\"📁 Model saved at: {model_dir}\")\n",
    "    print(f\"🚀 Ready for fine-tuning!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b4047",
   "metadata": {},
   "source": [
    "## Fine tuning the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4271b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
