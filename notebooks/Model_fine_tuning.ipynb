{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874a4af5",
   "metadata": {},
   "source": [
    "# Adjust the architecutre of pre-train model to our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03a2f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 08:47:16,605 - INFO - 🚀 Starting ViT Model Adjustment for Breast Cancer Detection\n",
      "2025-07-01 08:47:16,606 - INFO - Configuration initialized for breast_cancer_detection\n",
      "2025-07-01 08:47:16,607 - INFO - Target classes: 2 (normal, cancer)\n",
      "2025-07-01 08:47:16,607 - INFO - Device: cpu\n",
      "2025-07-01 08:47:16,607 - INFO - Starting ViT model adjustment process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 ViT Model Adjustment for Breast Cancer Detection\n",
      "============================================================\n",
      "This script will adjust the ViT model architecture for binary classification.\n",
      "The model will be ready for fine-tuning on your breast cancer dataset.\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e68ddfb5524d2c80d9013d0773e564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Adjustment Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 08:47:16,628 - INFO - Loading original ViT model and processor...\n",
      "2025-07-01 08:47:16,637 - INFO - Loading from local path: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/raw_model\n",
      "2025-07-01 08:47:16,884 - INFO - Original model loaded successfully\n",
      "2025-07-01 08:47:16,884 - INFO - Original number of classes: 1000\n",
      "2025-07-01 08:47:16,885 - INFO - Model architecture: ['ViTForImageClassification']\n",
      "2025-07-01 08:47:16,886 - INFO - \n",
      "==================================================\n",
      "2025-07-01 08:47:16,886 - INFO - Original Model Information\n",
      "2025-07-01 08:47:16,887 - INFO - ==================================================\n",
      "2025-07-01 08:47:16,888 - INFO - Architecture: ['ViTForImageClassification']\n",
      "2025-07-01 08:47:16,888 - INFO - Number of labels: 1000\n",
      "2025-07-01 08:47:16,889 - INFO - Hidden size: 768\n",
      "2025-07-01 08:47:16,889 - INFO - Number of attention heads: 12\n",
      "2025-07-01 08:47:16,890 - INFO - Number of layers: 12\n",
      "2025-07-01 08:47:16,890 - INFO - Image size: 224\n",
      "2025-07-01 08:47:16,891 - INFO - Patch size: 16\n",
      "2025-07-01 08:47:16,891 - INFO - Classifier input features: 768\n",
      "2025-07-01 08:47:16,892 - INFO - Classifier output features: 1000\n",
      "2025-07-01 08:47:16,893 - INFO - Total parameters: 86,567,656\n",
      "2025-07-01 08:47:16,893 - INFO - Trainable parameters: 86,567,656\n",
      "2025-07-01 08:47:16,893 - INFO - ==================================================\n",
      "\n",
      "2025-07-01 08:47:16,894 - INFO - Adjusting model architecture for breast cancer classification...\n",
      "2025-07-01 08:47:16,899 - INFO - Creating adjusted model with new configuration...\n",
      "2025-07-01 08:47:18,424 - INFO - Copying pre-trained weights...\n",
      "2025-07-01 08:47:20,193 - INFO - Copied weights for 198 layers\n",
      "2025-07-01 08:47:20,194 - INFO - Skipped 2 layers:\n",
      "2025-07-01 08:47:20,194 - INFO -   - classifier.weight (classifier layer)\n",
      "2025-07-01 08:47:20,195 - INFO -   - classifier.bias (classifier layer)\n",
      "2025-07-01 08:47:20,196 - INFO - Initializing new classifier layer...\n",
      "2025-07-01 08:47:20,197 - INFO - Classifier initialized - Input: 768, Output: 2\n",
      "2025-07-01 08:47:20,197 - INFO - Model architecture adjusted successfully!\n",
      "2025-07-01 08:47:20,198 - INFO - \n",
      "==================================================\n",
      "2025-07-01 08:47:20,199 - INFO - Adjusted Model Information\n",
      "2025-07-01 08:47:20,199 - INFO - ==================================================\n",
      "2025-07-01 08:47:20,200 - INFO - Architecture: ['ViTForImageClassification']\n",
      "2025-07-01 08:47:20,200 - INFO - Number of labels: 2\n",
      "2025-07-01 08:47:20,201 - INFO - Hidden size: 768\n",
      "2025-07-01 08:47:20,201 - INFO - Number of attention heads: 12\n",
      "2025-07-01 08:47:20,201 - INFO - Number of layers: 12\n",
      "2025-07-01 08:47:20,201 - INFO - Image size: 224\n",
      "2025-07-01 08:47:20,202 - INFO - Patch size: 16\n",
      "2025-07-01 08:47:20,202 - INFO - Classifier input features: 768\n",
      "2025-07-01 08:47:20,202 - INFO - Classifier output features: 2\n",
      "2025-07-01 08:47:20,203 - INFO - Total parameters: 85,800,194\n",
      "2025-07-01 08:47:20,203 - INFO - Trainable parameters: 85,800,194\n",
      "2025-07-01 08:47:20,203 - INFO - ==================================================\n",
      "\n",
      "2025-07-01 08:47:20,204 - INFO - Validating adjusted model...\n",
      "2025-07-01 08:47:20,322 - INFO - ✓ Model validation successful! Output shape: torch.Size([1, 2])\n",
      "2025-07-01 08:47:20,326 - INFO - ✓ Sample output probabilities: [0.18248295783996582, 0.8175170421600342]\n",
      "2025-07-01 08:47:20,354 - INFO - Created output directory: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:20,354 - INFO - Saving adjusted model to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:21,955 - INFO - ✓ Model saved successfully\n",
      "2025-07-01 08:47:22,000 - INFO - ✓ Processor saved successfully\n",
      "2025-07-01 08:47:22,024 - INFO - ✓ Metadata saved to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted/adjustment_metadata.json\n",
      "2025-07-01 08:47:22,056 - INFO - ✓ README created: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted/README.md\n",
      "2025-07-01 08:47:22,057 - INFO - Verifying saved model...\n",
      "2025-07-01 08:47:23,654 - INFO - ✅ Saved model verification successful!\n",
      "2025-07-01 08:47:23,656 - INFO - ✅ Model adjustment completed successfully!\n",
      "2025-07-01 08:47:23,657 - INFO - Adjusted model saved to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:23,657 - INFO - 🎉 Process completed successfully!\n",
      "2025-07-01 08:47:23,657 - INFO - 📁 Adjusted model location: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n",
      "2025-07-01 08:47:23,658 - INFO - 📋 Next steps:\n",
      "2025-07-01 08:47:23,658 - INFO -    1. Use this adjusted model for fine-tuning on your dataset\n",
      "2025-07-01 08:47:23,658 - INFO -    2. The model is ready for training with your breast cancer data\n",
      "2025-07-01 08:47:23,659 - INFO -    3. Check the README.md file for usage instructions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SUCCESS: Model adjusted and saved to: /Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model/breast_cancer_vit_adjusted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    ViTImageProcessor, \n",
    "    ViTForImageClassification,\n",
    "    ViTConfig\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('model_adjustment.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModelAdjustmentConfig:\n",
    "    \"\"\"Configuration class for model architecture adjustment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model paths\n",
    "        self.base_model_path = '/Volumes/KODAK/folder 02/Brest_cancer_prediction/model/raw_model'\n",
    "        self.output_model_path = '/Volumes/KODAK/folder 02/Brest_cancer_prediction/model/fine_tuning_model'\n",
    "        self.adjusted_model_name = 'breast_cancer_vit_adjusted'\n",
    "        \n",
    "        # Task-specific parameters\n",
    "        self.num_classes = 2  # Binary classification: cancer/no cancer\n",
    "        self.task_name = 'breast_cancer_detection'\n",
    "        self.class_names = ['normal', 'cancer']\n",
    "        \n",
    "        # Model configuration\n",
    "        self.dropout_rate = 0.1\n",
    "        self.label_smoothing = 0.1\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        logger.info(f\"Configuration initialized for {self.task_name}\")\n",
    "        logger.info(f\"Target classes: {self.num_classes} ({', '.join(self.class_names)})\")\n",
    "        logger.info(f\"Device: {self.device}\")\n",
    "\n",
    "class ViTModelAdjuster:\n",
    "    \"\"\"Class for adjusting ViT model architecture for specific tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelAdjustmentConfig):\n",
    "        self.config = config\n",
    "        self.processor = None\n",
    "        self.original_model = None\n",
    "        self.adjusted_model = None\n",
    "        self.model_config = None\n",
    "        \n",
    "    def create_output_directory(self):\n",
    "        \"\"\"Create output directory structure\"\"\"\n",
    "        output_path = Path(self.config.output_model_path)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        adjusted_model_path = output_path / self.config.adjusted_model_name\n",
    "        adjusted_model_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Created output directory: {adjusted_model_path}\")\n",
    "        return str(adjusted_model_path)\n",
    "    \n",
    "    def load_original_model(self):\n",
    "        \"\"\"Load the original pre-trained model\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Loading original ViT model and processor...\")\n",
    "            \n",
    "            # Check if local model exists\n",
    "            if os.path.exists(self.config.base_model_path):\n",
    "                logger.info(f\"Loading from local path: {self.config.base_model_path}\")\n",
    "                \n",
    "                # Load processor\n",
    "                self.processor = ViTImageProcessor.from_pretrained(self.config.base_model_path)\n",
    "                \n",
    "                # Load model configuration\n",
    "                self.model_config = ViTConfig.from_pretrained(self.config.base_model_path)\n",
    "                \n",
    "                # Load original model\n",
    "                self.original_model = ViTForImageClassification.from_pretrained(\n",
    "                    self.config.base_model_path\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                logger.info(\"Local model not found. Loading from Hugging Face...\")\n",
    "                \n",
    "                # Load processor\n",
    "                self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "                \n",
    "                # Load model configuration\n",
    "                self.model_config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "                \n",
    "                # Load original model\n",
    "                self.original_model = ViTForImageClassification.from_pretrained(\n",
    "                    'google/vit-base-patch16-224'\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Original model loaded successfully\")\n",
    "            logger.info(f\"Original number of classes: {self.model_config.num_labels}\")\n",
    "            logger.info(f\"Model architecture: {self.model_config.architectures}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading original model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def print_model_info(self, model, title=\"Model Information\"):\n",
    "        \"\"\"Print detailed model information\"\"\"\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"{title}\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "        \n",
    "        # Model configuration\n",
    "        config = model.config\n",
    "        logger.info(f\"Architecture: {config.architectures}\")\n",
    "        logger.info(f\"Number of labels: {config.num_labels}\")\n",
    "        logger.info(f\"Hidden size: {config.hidden_size}\")\n",
    "        logger.info(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "        logger.info(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "        logger.info(f\"Image size: {config.image_size}\")\n",
    "        logger.info(f\"Patch size: {config.patch_size}\")\n",
    "        \n",
    "        # Classifier layer info\n",
    "        if hasattr(model, 'classifier'):\n",
    "            classifier = model.classifier\n",
    "            logger.info(f\"Classifier input features: {classifier.in_features}\")\n",
    "            logger.info(f\"Classifier output features: {classifier.out_features}\")\n",
    "        \n",
    "        # Model parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        logger.info(f\"Total parameters: {total_params:,}\")\n",
    "        logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        logger.info(f\"{'='*50}\\n\")\n",
    "    \n",
    "    def adjust_model_architecture(self):\n",
    "        \"\"\"Adjust model architecture for the target task\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Adjusting model architecture for breast cancer classification...\")\n",
    "            \n",
    "            # Create new configuration with adjusted parameters\n",
    "            new_config = ViTConfig.from_pretrained(\n",
    "                self.config.base_model_path if os.path.exists(self.config.base_model_path) \n",
    "                else 'google/vit-base-patch16-224'\n",
    "            )\n",
    "            \n",
    "            # Update configuration for the new task\n",
    "            new_config.num_labels = self.config.num_classes\n",
    "            new_config.id2label = {i: label for i, label in enumerate(self.config.class_names)}\n",
    "            new_config.label2id = {label: i for i, label in enumerate(self.config.class_names)}\n",
    "            new_config.problem_type = \"single_label_classification\"\n",
    "            \n",
    "            # Update dropout rates for better regularization\n",
    "            new_config.hidden_dropout_prob = self.config.hidden_dropout_prob\n",
    "            new_config.attention_probs_dropout_prob = self.config.attention_probs_dropout_prob\n",
    "            \n",
    "            # Add task-specific metadata\n",
    "            new_config.task_specific_params = {\n",
    "                \"task_name\": self.config.task_name,\n",
    "                \"num_classes\": self.config.num_classes,\n",
    "                \"class_names\": self.config.class_names,\n",
    "                \"adjustment_date\": datetime.now().isoformat(),\n",
    "                \"base_model\": \"google/vit-base-patch16-224\"\n",
    "            }\n",
    "            \n",
    "            logger.info(\"Creating adjusted model with new configuration...\")\n",
    "            \n",
    "            # Create new model with adjusted configuration\n",
    "            self.adjusted_model = ViTForImageClassification(new_config)\n",
    "            \n",
    "            # Copy weights from original model (except classifier layer)\n",
    "            self.copy_pretrained_weights()\n",
    "            \n",
    "            # Initialize new classifier layer\n",
    "            self.initialize_classifier_layer()\n",
    "            \n",
    "            logger.info(\"Model architecture adjusted successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adjusting model architecture: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def copy_pretrained_weights(self):\n",
    "        \"\"\"Copy weights from original model to adjusted model (except classifier)\"\"\"\n",
    "        logger.info(\"Copying pre-trained weights...\")\n",
    "        \n",
    "        # Get state dictionaries\n",
    "        original_state_dict = self.original_model.state_dict()\n",
    "        adjusted_state_dict = self.adjusted_model.state_dict()\n",
    "        \n",
    "        # Copy all weights except classifier\n",
    "        copied_layers = []\n",
    "        skipped_layers = []\n",
    "        \n",
    "        for name, param in original_state_dict.items():\n",
    "            if name in adjusted_state_dict and not name.startswith('classifier'):\n",
    "                if param.shape == adjusted_state_dict[name].shape:\n",
    "                    adjusted_state_dict[name].copy_(param)\n",
    "                    copied_layers.append(name)\n",
    "                else:\n",
    "                    skipped_layers.append(f\"{name} (shape mismatch)\")\n",
    "            elif name.startswith('classifier'):\n",
    "                skipped_layers.append(f\"{name} (classifier layer)\")\n",
    "            else:\n",
    "                skipped_layers.append(f\"{name} (not found in target)\")\n",
    "        \n",
    "        # Load the updated state dict\n",
    "        self.adjusted_model.load_state_dict(adjusted_state_dict)\n",
    "        \n",
    "        logger.info(f\"Copied weights for {len(copied_layers)} layers\")\n",
    "        logger.info(f\"Skipped {len(skipped_layers)} layers:\")\n",
    "        for layer in skipped_layers[:5]:  # Show first 5 skipped layers\n",
    "            logger.info(f\"  - {layer}\")\n",
    "        if len(skipped_layers) > 5:\n",
    "            logger.info(f\"  ... and {len(skipped_layers) - 5} more\")\n",
    "    \n",
    "    def initialize_classifier_layer(self):\n",
    "        \"\"\"Initialize the new classifier layer with proper weights\"\"\"\n",
    "        logger.info(\"Initializing new classifier layer...\")\n",
    "        \n",
    "        # Get the classifier layer\n",
    "        classifier = self.adjusted_model.classifier\n",
    "        \n",
    "        # Initialize with Xavier/Glorot initialization\n",
    "        nn.init.xavier_uniform_(classifier.weight)\n",
    "        nn.init.constant_(classifier.bias, 0)\n",
    "        \n",
    "        logger.info(f\"Classifier initialized - Input: {classifier.in_features}, Output: {classifier.out_features}\")\n",
    "    \n",
    "    def validate_adjusted_model(self):\n",
    "        \"\"\"Validate the adjusted model\"\"\"\n",
    "        logger.info(\"Validating adjusted model...\")\n",
    "        \n",
    "        try:\n",
    "            # Create dummy input\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            \n",
    "            # Test forward pass\n",
    "            self.adjusted_model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.adjusted_model(pixel_values=dummy_input)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # Validate output shape\n",
    "            expected_shape = (1, self.config.num_classes)\n",
    "            actual_shape = logits.shape\n",
    "            \n",
    "            if actual_shape == expected_shape:\n",
    "                logger.info(f\"✓ Model validation successful! Output shape: {actual_shape}\")\n",
    "                \n",
    "                # Test probability distribution\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                logger.info(f\"✓ Sample output probabilities: {probabilities.squeeze().tolist()}\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"✗ Model validation failed! Expected shape: {expected_shape}, Got: {actual_shape}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Model validation failed with error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def save_adjusted_model(self):\n",
    "        \"\"\"Save the adjusted model and processor\"\"\"\n",
    "        try:\n",
    "            # Create output directory\n",
    "            output_dir = self.create_output_directory()\n",
    "            \n",
    "            logger.info(f\"Saving adjusted model to: {output_dir}\")\n",
    "            \n",
    "            # Save model\n",
    "            self.adjusted_model.save_pretrained(output_dir)\n",
    "            logger.info(\"✓ Model saved successfully\")\n",
    "            \n",
    "            # Save processor\n",
    "            self.processor.save_pretrained(output_dir)\n",
    "            logger.info(\"✓ Processor saved successfully\")\n",
    "            \n",
    "            # Save adjustment metadata\n",
    "            metadata = {\n",
    "                \"adjustment_info\": {\n",
    "                    \"original_model\": \"google/vit-base-patch16-224\",\n",
    "                    \"task\": self.config.task_name,\n",
    "                    \"num_classes\": self.config.num_classes,\n",
    "                    \"class_names\": self.config.class_names,\n",
    "                    \"adjustment_date\": datetime.now().isoformat()\n",
    "                },\n",
    "                \"model_config\": {\n",
    "                    \"hidden_size\": self.adjusted_model.config.hidden_size,\n",
    "                    \"num_attention_heads\": self.adjusted_model.config.num_attention_heads,\n",
    "                    \"num_hidden_layers\": self.adjusted_model.config.num_hidden_layers,\n",
    "                    \"image_size\": self.adjusted_model.config.image_size,\n",
    "                    \"patch_size\": self.adjusted_model.config.patch_size,\n",
    "                    \"num_labels\": self.adjusted_model.config.num_labels\n",
    "                },\n",
    "                \"usage_instructions\": {\n",
    "                    \"loading\": \"Use ViTForImageClassification.from_pretrained() to load this model\",\n",
    "                    \"processor\": \"Use ViTImageProcessor.from_pretrained() to load the processor\",\n",
    "                    \"input_size\": \"224x224 RGB images\",\n",
    "                    \"output\": f\"{self.config.num_classes} class probabilities\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            metadata_path = os.path.join(output_dir, 'adjustment_metadata.json')\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"✓ Metadata saved to: {metadata_path}\")\n",
    "            \n",
    "            # Create README file\n",
    "            readme_content = self.create_readme_content(output_dir)\n",
    "            readme_path = os.path.join(output_dir, 'README.md')\n",
    "            with open(readme_path, 'w') as f:\n",
    "                f.write(readme_content)\n",
    "            \n",
    "            logger.info(f\"✓ README created: {readme_path}\")\n",
    "            \n",
    "            return output_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving adjusted model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_readme_content(self, model_path):\n",
    "        \"\"\"Create README content for the adjusted model\"\"\"\n",
    "        readme = f\"\"\"# Breast Cancer Detection ViT Model (Adjusted)\n",
    "\n",
    "## Model Information\n",
    "- **Base Model**: google/vit-base-patch16-224\n",
    "- **Task**: Binary classification for breast cancer detection\n",
    "- **Classes**: {self.config.num_classes} ({', '.join(self.config.class_names)})\n",
    "- **Adjustment Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Model Architecture\n",
    "- **Input Size**: 224x224 RGB images\n",
    "- **Hidden Size**: {self.adjusted_model.config.hidden_size}\n",
    "- **Attention Heads**: {self.adjusted_model.config.num_attention_heads}\n",
    "- **Layers**: {self.adjusted_model.config.num_hidden_layers}\n",
    "- **Parameters**: {sum(p.numel() for p in self.adjusted_model.parameters()):,}\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Loading the Model\n",
    "```python\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Load model and processor\n",
    "model = ViTForImageClassification.from_pretrained('{model_path}')\n",
    "processor = ViTImageProcessor.from_pretrained('{model_path}')\n",
    "```\n",
    "\n",
    "### Inference Example\n",
    "```python\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess image\n",
    "image = Image.open('path/to/mri_image.jpg')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class_id = predictions.argmax().item()\n",
    "predicted_class = {self.config.class_names}[predicted_class_id]\n",
    "confidence = predictions[0][predicted_class_id].item()\n",
    "\n",
    "print(f\"Prediction: {{predicted_class}} (Confidence: {{confidence:.4f}})\")\n",
    "```\n",
    "\n",
    "## Fine-tuning Ready\n",
    "This model is ready for fine-tuning on your breast cancer dataset. The classifier layer has been properly initialized for binary classification.\n",
    "\n",
    "## Notes\n",
    "- The model uses the original ViT-Base architecture with an adjusted classifier layer\n",
    "- All pre-trained weights from ImageNet are preserved except for the final classification layer\n",
    "- The classifier layer has been randomly initialized and needs fine-tuning on your dataset\n",
    "\"\"\"\n",
    "        return readme\n",
    "    \n",
    "    def run_adjustment_process(self):\n",
    "        \"\"\"Run the complete model adjustment process\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting ViT model adjustment process...\")\n",
    "            \n",
    "            # Step 1: Load original model\n",
    "            with tqdm(total=5, desc=\"Model Adjustment Progress\") as pbar:\n",
    "                self.load_original_model()\n",
    "                pbar.set_description(\"Original model loaded\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Print original model info\n",
    "                self.print_model_info(self.original_model, \"Original Model Information\")\n",
    "                \n",
    "                # Step 2: Adjust architecture\n",
    "                self.adjust_model_architecture()\n",
    "                pbar.set_description(\"Architecture adjusted\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Print adjusted model info\n",
    "                self.print_model_info(self.adjusted_model, \"Adjusted Model Information\")\n",
    "                \n",
    "                # Step 3: Validate adjusted model\n",
    "                validation_success = self.validate_adjusted_model()\n",
    "                if not validation_success:\n",
    "                    raise Exception(\"Model validation failed!\")\n",
    "                pbar.set_description(\"Model validated\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Step 4: Save adjusted model\n",
    "                output_path = self.save_adjusted_model()\n",
    "                pbar.set_description(\"Model saved\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Step 5: Final verification\n",
    "                self.verify_saved_model(output_path)\n",
    "                pbar.set_description(\"Verification complete\")\n",
    "                pbar.update(1)\n",
    "            \n",
    "            logger.info(\"✅ Model adjustment completed successfully!\")\n",
    "            logger.info(f\"Adjusted model saved to: {output_path}\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Model adjustment failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def verify_saved_model(self, model_path):\n",
    "        \"\"\"Verify that the saved model can be loaded correctly\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Verifying saved model...\")\n",
    "            \n",
    "            # Load the saved model\n",
    "            loaded_model = ViTForImageClassification.from_pretrained(model_path)\n",
    "            loaded_processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "            \n",
    "            # Test with dummy input\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            \n",
    "            loaded_model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = loaded_model(pixel_values=dummy_input)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            if logits.shape == (1, self.config.num_classes):\n",
    "                logger.info(\"✅ Saved model verification successful!\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"❌ Saved model verification failed!\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error verifying saved model: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    logger.info(\"🚀 Starting ViT Model Adjustment for Breast Cancer Detection\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        config = ModelAdjustmentConfig()\n",
    "        \n",
    "        # Initialize model adjuster\n",
    "        adjuster = ViTModelAdjuster(config)\n",
    "        \n",
    "        # Run adjustment process\n",
    "        output_path = adjuster.run_adjustment_process()\n",
    "        \n",
    "        logger.info(\"🎉 Process completed successfully!\")\n",
    "        logger.info(f\"📁 Adjusted model location: {output_path}\")\n",
    "        logger.info(\"📋 Next steps:\")\n",
    "        logger.info(\"   1. Use this adjusted model for fine-tuning on your dataset\")\n",
    "        logger.info(\"   2. The model is ready for training with your breast cancer data\")\n",
    "        logger.info(\"   3. Check the README.md file for usage instructions\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"💥 Process failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# For Jupyter Notebook usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔬 ViT Model Adjustment for Breast Cancer Detection\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"This script will adjust the ViT model architecture for binary classification.\")\n",
    "    print(\"The model will be ready for fine-tuning on your breast cancer dataset.\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run the adjustment process\n",
    "    try:\n",
    "        output_path = main()\n",
    "        print(f\"\\n✅ SUCCESS: Model adjusted and saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {str(e)}\")\n",
    "        print(\"Please check the logs for more details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c3622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
