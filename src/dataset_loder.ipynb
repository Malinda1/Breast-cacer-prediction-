{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset from Kaggle ('olegbaryshnikov/rsna-roi-512x512-pngs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 09:11:14,958 - INFO - Initializing Kaggle API connection...\n",
      "2025-06-22 09:11:14,960 - INFO - Downloading dataset: olegbaryshnikov/rsna-roi-512x512-pngs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/olegbaryshnikov/rsna-roi-512x512-pngs\n",
      "Downloading rsna-roi-512x512-pngs.zip to /Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.21G/9.21G [01:22<00:00, 120MB/s] \n",
      "2025-06-22 10:38:13,530 - INFO - Successfully downloaded dataset to /Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data/olegbaryshnikov_rsna-roi-512x512-pngs.zip\n",
      "2025-06-22 10:38:13,535 - INFO - Extracting olegbaryshnikov_rsna-roi-512x512-pngs.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 12.1G/12.1G [17:22<00:00, 11.6MB/s] \n",
      "2025-06-22 10:55:36,087 - INFO - Extraction complete to /Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data\n",
      "2025-06-22 10:55:36,101 - INFO - Deleted zip file: olegbaryshnikov_rsna-roi-512x512-pngs.zip\n",
      "2025-06-22 10:55:36,203 - INFO - Dataset download and processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('kaggle_downloader.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class KaggleDatasetDownloader:\n",
    "    def __init__(self, output_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize the Kaggle dataset downloader.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str): Directory where the dataset will be saved.\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.api = KaggleApi()\n",
    "        \n",
    "        # Set custom kaggle.json location if needed\n",
    "        if not (Path.home() / '.kaggle' / 'kaggle.json').exists():\n",
    "            custom_kaggle_path = '/Volumes/KODAK/folder 02/Brest_cancer_prediction/src/kaggle.json'\n",
    "            if Path(custom_kaggle_path).exists():\n",
    "                os.environ['KAGGLE_CONFIG_DIR'] = str(Path(custom_kaggle_path).parent)\n",
    "        \n",
    "        # Validate and create output directory\n",
    "        self._prepare_output_directory()\n",
    "        \n",
    "    def _prepare_output_directory(self) -> None:\n",
    "        \"\"\"Ensure the output directory exists and is writable.\"\"\"\n",
    "        try:\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            # Test write permission\n",
    "            test_file = self.output_dir / '.permission_test'\n",
    "            test_file.touch()\n",
    "            test_file.unlink()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to prepare output directory: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _validate_kaggle_credentials(self) -> bool:\n",
    "        \"\"\"Check if Kaggle credentials are properly configured.\"\"\"\n",
    "        try:\n",
    "            kaggle_dir = Path(os.environ.get('KAGGLE_CONFIG_DIR', Path.home() / '.kaggle'))\n",
    "            kaggle_json = kaggle_dir / 'kaggle.json'\n",
    "            \n",
    "            if not kaggle_json.exists():\n",
    "                logger.error(f\"Kaggle credentials not found at {kaggle_json}. Please ensure kaggle.json exists.\")\n",
    "                return False\n",
    "                \n",
    "            with open(kaggle_json) as f:\n",
    "                json.load(f)  # Validate JSON\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Invalid kaggle.json file: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def _download_with_progress(self, dataset_name: str, destination: Path) -> bool:\n",
    "        \"\"\"Download dataset with progress bar.\"\"\"\n",
    "        try:\n",
    "            # Get the download URL\n",
    "            dataset_files = self.api.dataset_list_files(dataset_name).files\n",
    "            if not dataset_files:\n",
    "                logger.error(\"No files found in dataset\")\n",
    "                return False\n",
    "                \n",
    "            # Create progress bar\n",
    "            with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1) as pbar:\n",
    "                def update_progress(block_num, block_size, total_size):\n",
    "                    if pbar.total != total_size:\n",
    "                        pbar.total = total_size\n",
    "                    pbar.update(block_size)\n",
    "                \n",
    "                # Download each file in the dataset\n",
    "                for file in dataset_files:\n",
    "                    file_path = destination / file.name\n",
    "                    logger.info(f\"Downloading {file.name}...\")\n",
    "                    \n",
    "                    self.api.dataset_download_file(\n",
    "                        dataset=dataset_name,\n",
    "                        file_name=file.name,\n",
    "                        path=destination,\n",
    "                        force=True,\n",
    "                        quiet=True\n",
    "                    )\n",
    "                    \n",
    "                    # The API doesn't provide direct progress, so we simulate it\n",
    "                    temp_file = destination / file.name\n",
    "                    if temp_file.exists():\n",
    "                        temp_file.rename(file_path)\n",
    "                        pbar.total = os.path.getsize(file_path)\n",
    "                        pbar.update(os.path.getsize(file_path))\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Download failed: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def download_dataset(self, dataset_name: str, unzip: bool = True, delete_zip: bool = True) -> bool:\n",
    "        \"\"\"\n",
    "        Download a dataset from Kaggle.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Kaggle dataset identifier in format 'owner/dataset-name'\n",
    "            unzip (bool): Whether to unzip the downloaded file\n",
    "            delete_zip (bool): Whether to delete the zip file after extraction\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if download and processing succeeded, False otherwise\n",
    "        \"\"\"\n",
    "        if not self._validate_kaggle_credentials():\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            logger.info(f\"Initializing Kaggle API connection...\")\n",
    "            self.api.authenticate()\n",
    "            \n",
    "            logger.info(f\"Downloading dataset: {dataset_name}\")\n",
    "            \n",
    "            # Download with progress tracking\n",
    "            zip_path = self.output_dir / f\"{dataset_name.replace('/', '_')}.zip\"\n",
    "            \n",
    "            # First try the standard download method\n",
    "            try:\n",
    "                self.api.dataset_download_files(\n",
    "                    dataset=dataset_name,\n",
    "                    path=self.output_dir,\n",
    "                    quiet=False,  # Let Kaggle show its progress\n",
    "                    force=True,\n",
    "                    unzip=False\n",
    "                )\n",
    "                \n",
    "                # Rename the downloaded file to a consistent format\n",
    "                temp_zip = self.output_dir / f\"{dataset_name.split('/')[1]}.zip\"\n",
    "                if temp_zip.exists():\n",
    "                    temp_zip.rename(zip_path)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Standard download failed, trying alternative method: {e}\")\n",
    "                if not self._download_with_progress(dataset_name, self.output_dir):\n",
    "                    raise RuntimeError(\"Both download methods failed\")\n",
    "                \n",
    "            if not zip_path.exists():\n",
    "                # Check if files were downloaded without zip\n",
    "                contents = list(self.output_dir.glob('*'))\n",
    "                if contents:\n",
    "                    logger.info(f\"Files downloaded directly without zip: {contents}\")\n",
    "                    return True\n",
    "                raise FileNotFoundError(f\"Downloaded files not found at {self.output_dir}\")\n",
    "                \n",
    "            logger.info(f\"Successfully downloaded dataset to {zip_path}\")\n",
    "            \n",
    "            if unzip and zip_path.exists():\n",
    "                self._unzip_file(zip_path, delete_zip)\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logger.error(f\"HTTP Error occurred: {e}\")\n",
    "            if e.response.status_code == 403:\n",
    "                logger.error(\"Authentication failed. Please check your Kaggle API token.\")\n",
    "            elif e.response.status_code == 404:\n",
    "                logger.error(\"Dataset not found. Please check the dataset name.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while downloading dataset: {e}\")\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    def _unzip_file(self, zip_path: Path, delete_zip: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Unzip a downloaded dataset.\n",
    "        \n",
    "        Args:\n",
    "            zip_path (Path): Path to the zip file\n",
    "            delete_zip (bool): Whether to delete the zip file after extraction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Extracting {zip_path.name}...\")\n",
    "            \n",
    "            # Get total size for progress bar\n",
    "            total_size = sum(f.file_size for f in zipfile.ZipFile(zip_path).infolist())\n",
    "            \n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Extracting\") as pbar:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    for file in zip_ref.infolist():\n",
    "                        try:\n",
    "                            zip_ref.extract(file, self.output_dir)\n",
    "                            pbar.update(file.file_size)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to extract {file.filename}: {e}\")\n",
    "                            continue\n",
    "                        \n",
    "            logger.info(f\"Extraction complete to {self.output_dir}\")\n",
    "            \n",
    "            if delete_zip:\n",
    "                zip_path.unlink()\n",
    "                logger.info(f\"Deleted zip file: {zip_path.name}\")\n",
    "                \n",
    "        except zipfile.BadZipFile:\n",
    "            logger.error(f\"File is not a zip file or is corrupted: {zip_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during extraction: {e}\")\n",
    "            \n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    DATASET_NAME = \"olegbaryshnikov/rsna-roi-512x512-pngs\"\n",
    "    OUTPUT_DIR = \"/Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data\"\n",
    "    \n",
    "    try:\n",
    "        downloader = KaggleDatasetDownloader(OUTPUT_DIR)\n",
    "        success = downloader.download_dataset(DATASET_NAME)\n",
    "        \n",
    "        if success:\n",
    "            logger.info(\"Dataset download and processing completed successfully!\")\n",
    "        else:\n",
    "            logger.error(\"Dataset download failed.\")\n",
    "            exit(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in main execution: {e}\")\n",
    "        exit(1)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CSV files in: /Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data\n",
      "Found 1 CSV file(s):\n",
      "  - unrecognized_images.csv\n",
      "\n",
      "Analyzing: unrecognized_images.csv\n",
      "  - Shape: (89, 3) (rows, columns)\n",
      "  - Columns: ['Unnamed: 0', 'patient_id', 'image_id']\n",
      "  - Found relevant columns: ['patient_id', 'image_id']\n",
      "  - Value counts for 'patient_id':\n",
      "33581    4\n",
      "36584    4\n",
      "2738     4\n",
      "13095    3\n",
      "52509    3\n",
      "39850    3\n",
      "59101    3\n",
      "20008    3\n",
      "60669    3\n",
      "65471    2\n",
      "735      2\n",
      "7010     2\n",
      "4073     2\n",
      "4659     2\n",
      "51985    2\n",
      "53879    2\n",
      "32292    2\n",
      "3768     2\n",
      "16497    2\n",
      "26102    2\n",
      "26576    1\n",
      "8421     1\n",
      "7330     1\n",
      "822      1\n",
      "17111    1\n",
      "43368    1\n",
      "44259    1\n",
      "46373    1\n",
      "489      1\n",
      "2086     1\n",
      "50601    1\n",
      "51028    1\n",
      "16124    1\n",
      "15503    1\n",
      "53470    1\n",
      "15237    1\n",
      "54713    1\n",
      "20302    1\n",
      "6637     1\n",
      "29768    1\n",
      "21827    1\n",
      "31065    1\n",
      "26530    1\n",
      "33150    1\n",
      "33208    1\n",
      "35039    1\n",
      "25578    1\n",
      "36847    1\n",
      "1511     1\n",
      "38571    1\n",
      "38703    1\n",
      "23419    1\n",
      "33084    1\n",
      "36590    1\n",
      "23251    1\n",
      "22573    1\n",
      "5509     1\n",
      "Name: patient_id, dtype: int64\n",
      "\n",
      "  Sample data:\n",
      "   Unnamed: 0  patient_id    image_id\n",
      "0           0       13095  1606501400\n",
      "1           1       13095   486827821\n",
      "2           2       13095  1525295982\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def check_for_csv_files(dataset_path):\n",
    "    \"\"\"\n",
    "    Check for CSV files in the dataset directory and validate their structure.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the raw dataset directory.\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    print(f\"Checking for CSV files in: {dataset_path}\")\n",
    "    \n",
    "    # Find all CSV files recursively (ignore macOS metadata files)\n",
    "    csv_files = [f for f in dataset_path.rglob(\"*.csv\") \n",
    "                if not f.name.startswith('._')]  # Skip macOS hidden files\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\" No valid CSV files found in the dataset directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"  - {csv_file.relative_to(dataset_path)}\")\n",
    "    \n",
    "    # Analyze each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"\\nAnalyzing: {csv_file.name}\")\n",
    "        try:\n",
    "            # Try multiple encodings for compatibility\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_file, encoding='latin1')\n",
    "            \n",
    "            # Basic info\n",
    "            print(f\"  - Shape: {df.shape} (rows, columns)\")\n",
    "            print(f\"  - Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Check for critical columns (case-insensitive)\n",
    "            critical_cols = {'image_path', 'label', 'diagnosis', 'patient_id', 'image_id'}\n",
    "            found_cols = [col for col in df.columns \n",
    "                         if col.lower() in {c.lower() for c in critical_cols}]\n",
    "            \n",
    "            if found_cols:\n",
    "                print(f\"  - Found relevant columns: {found_cols}\")\n",
    "                # Show value counts for the first matching column\n",
    "                print(f\"  - Value counts for '{found_cols[0]}':\\n{df[found_cols[0]].value_counts()}\")\n",
    "            else:\n",
    "                print(\"   No standard label columns found\")\n",
    "                print(\"     Expected columns like: 'image_path', 'label', 'diagnosis'\")\n",
    "            \n",
    "            # Display first 3 rows\n",
    "            print(\"\\n  Sample data:\")\n",
    "            print(df.head(3).to_string())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error reading {csv_file.name}: {str(e)}\")\n",
    "            if \"No columns to parse\" in str(e):\n",
    "                print(\"     This might be a corrupted or empty CSV file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"/Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data\"\n",
    "    check_for_csv_files(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully copied:\n",
      "  From: /Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data/unrecognized_images.csv\n",
      "  To: /Volumes/KODAK/folder 02/Brest_cancer_prediction/data/unrecognized_images.csv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def move_csv_file(source_dir, target_dir, csv_filename=\"unrecognized_images.csv\"):\n",
    "    \"\"\"\n",
    "    Move a CSV file from source directory to target directory.\n",
    "    \n",
    "    Args:\n",
    "        source_dir (str): Path to source directory containing the CSV\n",
    "        target_dir (str): Path to target directory\n",
    "        csv_filename (str): Name of the CSV file to move\n",
    "    \"\"\"\n",
    "    source_path = Path(source_dir) / csv_filename\n",
    "    target_path = Path(target_dir) / csv_filename\n",
    "    \n",
    "    try:\n",
    "        # Check if source exists\n",
    "        if not source_path.exists():\n",
    "            print(f\" Source file not found: {source_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Create target directory if needed\n",
    "        target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Copy the file (use shutil.copy2 to preserve metadata)\n",
    "        shutil.copy2(source_path, target_path)\n",
    "        print(f\"✅ Successfully copied:\\n\"\n",
    "              f\"  From: {source_path}\\n\"\n",
    "              f\"  To: {target_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error moving file: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define paths\n",
    "    source_directory = \"/Volumes/KODAK/folder 02/Brest_cancer_prediction/data/raw_data\"\n",
    "    target_directory = \"/Volumes/KODAK/folder 02/Brest_cancer_prediction/data\"\n",
    "    \n",
    "    # Execute the move\n",
    "    move_csv_file(source_directory, target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
